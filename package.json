{
  "name": "@duck4i/llama",
  "version": "0.1.2",
  "description": "Native Node.JS plugin to run LLAMA inference directly on your machine with no other dependencies. ",
  "main": "index.js",
  "module": "index.mjs",
  "scripts": {
    "install": "cmake-js compile",
    "rebuild": "cmake-js rebuild",
    "build": "cmake-js build",
    "clean": "cmake-js clean",
    "configure": "cmake-js reconfigure",
    "compile": "cmake-js compile",
    "test": "echo \"Error: no test specified\" && exit 1",
    "download": "node downloadModel.js",
    "inference": "node inference.js"
  },
  "bin": {
    "llama-download": "./downloadModel.js",
    "llama-run": "./inference.js"
  },
  "binary": {
    "napi_versions": [
      7
    ]
  },
  "keywords": [
    "llama",
    "node",
    "gguf",
    "LLM",
    "inference"
  ],
  "type": "commonjs",
  "author": "duck4i",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/duck4i/npm-llama.git"
  },
  "dependencies": {
    "axios": "^1.7.9",
    "bindings": "^1.5.0",
    "cmake-js": "^7.3.0",
    "commander": "^13.0.0",
    "node-addon-api": "^8.3.0"
  }
}